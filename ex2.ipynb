{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DLAD Exercise 2: Multitask Learning\n",
    "\n",
    "### Working with code\n",
    "\n",
    "- fork the original exercise template repository under your user account, make sure that the fork remains private\n",
    "- enable [mirroring](https://about.gitlab.com/blog/2016/12/01/how-to-keep-your-fork-up-to-date-with-its-origin/) so that the changes to the template (upstream repository) propagate into your fork. This is useful in case we find a bug in the template and push a fix to it - the fix will not propagate to your fork unless mirroring is enabled\n",
    "- add your fork to SageMaker repositories (see course presentation on AWS or check [here](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility/))\n",
    "- clone your fork to your PC, edit code, push changes back to your fork\n",
    "\n",
    "### Running experiments\n",
    "\n",
    "- The code is pulled from a linked repository _automatically_ only when you start the Notebook Instance. Subsequent pulls have to be done from the Terminal (see below)\n",
    "- The only cell in this notebook launches a new Training Job, and trains a model with it until the end\n",
    "- You can watch live TensorBoard every now and then to make sure training is doing fine. Just making sure the first training steps did not fail is enough in most cases\n",
    "- If you regret the hyperparameter choice, you can stop the job by visiting [SageMaker Training Jobs Console](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs), selecting the job in progress, and pressing Stop button. The job will take a couple of minutes to transition into Stopped state; until then your job is counted against your limits. After that you can stop the notebook cell. *Stopping the notebook cell alone does not terminate the Training Job, but only disconnects the notebook from it*\n",
    "- Each Training Job has a very simple lifecycle: it either runs until the end, thus resulting in a `submission.zip` file, or it fails/stops, in which case it cannot be resumed.\n",
    "\n",
    "### Incremental changes\n",
    "\n",
    "- (optional) With code changes\n",
    " - change code in your standalone environment\n",
    " - dry run code in your standalone environment to make sure it doesn't fail due to a syntax error, thus saving you time with Training Job initialization (order of 5 minutes)\n",
    " - push code to your private fork\n",
    " - open the Terminal within your Notebook Instance (Jupyter Logo in top-left corner -> New -> Terminal), navigate to your code folder\n",
    " - execute `git pull` to fetch the changes you pushed to your fork\n",
    "- (optional) With hyperparameter changes\n",
    " - update the hyperparameters dictionary in the cell below. It contains the command line parameters, which you would normally pass via command line as `--key <value>` in the standalone environment. All command line parameters are described in `mtl/utils/config.py`.\n",
    "- Assign a meaningful yet short name to your experiment. A good name allows you to quickly understand the meaning of the experiment when looking at a list with 20 other experiments. Example: `question1-lr-0.001-optimizer-adam`\n",
    "- Run the cell\n",
    "\n",
    "### Running multiple experiments simultaneously\n",
    "\n",
    "Each account is likely to have a limit of ml.p2.xlarge instances larger than 1, meaning that training multiple experiments in parallel is possible. To do that, we recommend having multiple (equal to your limit) separate Notebook Instances, each linked with the same repository. This way each of the Notebook Instances corresponds to one Training Job at time.\n",
    "\n",
    "## Start a Training Job\n",
    "\n",
    "- \"In [\\*]\" next to the cell means the Training Job is running\n",
    "- \"In [any digit]\" means the Trainin Job has finished, or the cell was disconnected from the job (either manually or by restarting/reconnecting to the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    # enable tensorboard server to run alongside the training code (you need this to track progress)\n",
    "    'tensorboard_daemon_start': True,\n",
    "    # enable forwarding tensorboard to your ngrok.com account (you need this to access tensorboard)\n",
    "    'ngrok_daemon_start': True,\n",
    "    # TODO: use your personal ngrok.com account authtoken to access tensorboard\n",
    "    'ngrok_auth_token': 'your token string here',\n",
    "    \n",
    "    # TODO: override configurable experiment parameters below this line\n",
    "    # 'batch_size': <new value>,\n",
    "    # 'num_epochs': ...,\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# TODO: give name to distinct experiments, e.g. 'batch-size-4--newideaX-on--someparam-off'\n",
    "experiment_name = 'experiment-meaningful-description'\n",
    "\n",
    "#\n",
    "# no changes below this line\n",
    "#\n",
    "import datetime\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir='./',\n",
    "    entry_point='train_sagemaker.py',\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    framework_version='1.4.0',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    train_volume_size=30,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_run=86000,\n",
    "    train_max_wait=86400,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    {'training': 's3://dlad-miniscapes/miniscapes.zip'},\n",
    "    job_name=experiment_name + datetime.datetime.now().strftime(\"-%Y-%m-%d-%H-%M-%S\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
