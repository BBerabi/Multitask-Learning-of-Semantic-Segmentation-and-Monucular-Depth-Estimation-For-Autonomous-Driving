{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DLAD Exercise 2: Multitask Learning\n",
    "\n",
    "### Working with code\n",
    "\n",
    "- fork the original exercise template repository under your user account, make sure that the fork remains private\n",
    "- [Set up an \"upstream\" remote of your fork](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/configuring-a-remote-for-a-fork) so that you can [pull in changes from the upstream repository](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) (template solution you forked) whenever we make important updates (announced via Piazza/email)\n",
    "- add your fork to SageMaker repositories (see course presentation on AWS or check [here](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility/))\n",
    "- clone your fork to your PC, edit code, push changes back to your fork\n",
    "\n",
    "### Running experiments\n",
    "\n",
    "- The code is pulled from a linked repository _automatically_ only when you start the Notebook Instance. Subsequent pulls have to be done from the Terminal (see below)\n",
    "- The only cell in this notebook launches a new Training Job, and trains a model with it until the end\n",
    "- You can watch live TensorBoard every now and then to make sure training is doing fine. Just making sure the first training steps did not fail is enough in most cases\n",
    "- If you regret the hyperparameter choice, you can stop the job by visiting [SageMaker Training Jobs Console](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs), selecting the job in progress, and pressing Stop button. The job will take a couple of minutes to transition into Stopped state; until then your job is counted against your limits. After that you can stop the notebook cell. *Stopping the notebook cell alone does not terminate the Training Job, but only disconnects the notebook from it*\n",
    "- Each Training Job has a very simple lifecycle: it either runs until the end, thus resulting in a `submission.zip` file, or it fails/stops, in which case it cannot be resumed.\n",
    "\n",
    "### Incremental changes\n",
    "\n",
    "- (optional) With code changes\n",
    " - change code in your standalone environment\n",
    " - dry run code in your standalone environment to make sure it doesn't fail due to a syntax error, thus saving you time with Training Job initialization (order of 5 minutes)\n",
    " - push code to your private fork\n",
    " - open the Terminal within your Notebook Instance (Jupyter Logo in top-left corner -> New -> Terminal), navigate to your code folder\n",
    " - execute `git pull` to fetch the changes you pushed to your fork\n",
    "- (optional) With hyperparameter changes\n",
    " - update the hyperparameters dictionary in the cell below. It contains the command line parameters, which you would normally pass via command line as `--key <value>` in the standalone environment. All command line parameters are described in `mtl/utils/config.py`.\n",
    "- Assign a meaningful yet short name to your experiment. A good name allows you to quickly understand the meaning of the experiment when looking at a list with 20 other experiments. Example: `question1-lr-0.001-optimizer-adam`\n",
    "- Run the cell\n",
    "\n",
    "### Running multiple experiments simultaneously\n",
    "\n",
    "Each account is likely to have a limit of ml.p2.xlarge instances larger than 1, meaning that training multiple experiments in parallel is possible. To do that, we recommend having multiple (equal to your limit) separate Notebook Instances, each linked with the same repository. This way each of the Notebook Instances corresponds to one Training Job at time.\n",
    "\n",
    "## Start a Training Job\n",
    "\n",
    "- \"In [\\*]\" next to the cell means the Training Job is running\n",
    "- \"In [any digit]\" means the Trainin Job has finished, or the cell was disconnected from the job (either manually or by restarting/reconnecting to the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23 18:26:28 Starting - Starting the training job...\n",
      "2020-04-23 18:26:30 Starting - Launching requested ML instances......\n",
      "2020-04-23 18:27:47 Starting - Preparing the instances for training.........\n",
      "2020-04-23 18:29:18 Downloading - Downloading input data.........\n",
      "2020-04-23 18:30:37 Training - Downloading the training image........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-04-23 18:32:08,480 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-04-23 18:32:08,505 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-04-23 18:32:08,512 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-04-23 18:32:09,556 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmptn5e2t8_/module_dir\u001b[0m\n",
      "\u001b[34mCollecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.17.3\u001b[0m\n",
      "\u001b[34m  Downloading numpy-1.17.3-cp36-cp36m-manylinux1_x86_64.whl (20.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting Pillow==6.2.1\n",
      "  Downloading Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==0.5.2.1\n",
      "  Downloading pytorch-lightning-0.5.2.1.tar.gz (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting test-tube==0.7.3\n",
      "  Downloading test_tube-0.7.3.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.5.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.5.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.36.1\n",
      "  Downloading tqdm-4.36.1-py2.py3-none-any.whl (52 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.8.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting tb-nightly==1.15.0a20190708\n",
      "  Downloading tb_nightly-1.15.0a20190708-py3-none-any.whl (3.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==0.5.0->-r requirements.txt (line 7)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\n",
      "2020-04-23 18:32:06 Training - Training image download completed. Training in progress.\u001b[34mCollecting grpcio>=1.6.3\n",
      "  Downloading grpcio-1.28.1-cp36-cp36m-manylinux2010_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pytorch-lightning, test-tube, DLAD-ex2-multitask, absl-py\n",
      "  Building wheel for pytorch-lightning (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for pytorch-lightning (setup.py): finished with status 'done'\n",
      "  Created wheel for pytorch-lightning: filename=pytorch_lightning-0.5.2.1-py3-none-any.whl size=62480 sha256=370044b88d514891d321379648a1c4ac1e1dd1b90c30d8679f37e46622f19a1a\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/25/4b/1d8ef595ef56b11055751ff6d70037c2bca8fe29ce097dee98\n",
      "  Building wheel for test-tube (setup.py): started\n",
      "  Building wheel for test-tube (setup.py): finished with status 'done'\n",
      "  Created wheel for test-tube: filename=test_tube-0.7.3-py3-none-any.whl size=25049 sha256=2bfab51bda12ac7484772faab55897c4949fadda1f9464b38fbb968b23f5c10b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/00/f4/dd4f6697d4925bfec3cd7ff8fed88d9930496d562454f5e70d\n",
      "  Building wheel for DLAD-ex2-multitask (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for DLAD-ex2-multitask (setup.py): finished with status 'done'\n",
      "  Created wheel for DLAD-ex2-multitask: filename=DLAD_ex2_multitask-1.0-py3-none-any.whl size=2067 sha256=7eb8fd9c29ba3f98a64c580918d853c010a0c870c08b5af5cacebebe95fb41a8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-q7q8x8xt/wheels/d9/a2/92/9518dd9a4efbc5d7fb3e1a10e113fde23e86f3908387a32221\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=087d6e6ef0fbde096e1b5539e68990f92e5876c079ba59d8590e3f169fcd6240\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[34mSuccessfully built pytorch-lightning test-tube DLAD-ex2-multitask absl-py\u001b[0m\n",
      "\u001b[34mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.36.1 which is incompatible.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, matplotlib, Pillow, tqdm, imageio, grpcio, markdown, absl-py, tb-nightly, test-tube, pytorch-lightning, DLAD-ex2-multitask\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.2.1\n",
      "    Uninstalling matplotlib-3.2.1:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled matplotlib-3.2.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 7.1.0\n",
      "    Uninstalling Pillow-7.1.0:\n",
      "      Successfully uninstalled Pillow-7.1.0\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed DLAD-ex2-multitask-1.0 Pillow-6.2.1 absl-py-0.9.0 grpcio-1.28.1 imageio-2.8.0 markdown-3.2.1 matplotlib-3.1.1 numpy-1.17.3 pytorch-lightning-0.5.2.1 tb-nightly-1.15.0a20190708 test-tube-0.7.3 tqdm-4.36.1\u001b[0m\n",
      "\u001b[34m2020-04-23 18:32:26,426 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib==3.1.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy==1.17.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow==6.2.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (6.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch-lightning==0.5.2.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.5.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: test-tube==0.7.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.5.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm==4.36.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tb-nightly==1.15.0a20190708 in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (1.15.0a20190708)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==0.5.0->-r requirements.txt (line 7)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.28.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: DLAD-ex2-multitask\n",
      "  Building wheel for DLAD-ex2-multitask (setup.py): started\n",
      "  Building wheel for DLAD-ex2-multitask (setup.py): finished with status 'done'\n",
      "  Created wheel for DLAD-ex2-multitask: filename=DLAD_ex2_multitask-1.0-py3-none-any.whl size=2067 sha256=96c9fe4bae6b3a84508cd6ca5c4a8c7ba75f360519e7d1ad57c611a7620bdf16\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_uek7lz4/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built DLAD-ex2-multitask\u001b[0m\n",
      "\u001b[34mInstalling collected packages: DLAD-ex2-multitask\n",
      "  Attempting uninstall: DLAD-ex2-multitask\n",
      "    Found existing installation: DLAD-ex2-multitask 1.0\n",
      "    Uninstalling DLAD-ex2-multitask-1.0:\n",
      "      Successfully uninstalled DLAD-ex2-multitask-1.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed DLAD-ex2-multitask-1.0\u001b[0m\n",
      "\u001b[34m2020-04-23 18:32:28,393 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"ngrok_daemon_start\": true,\n",
      "        \"tensorboard_daemon_start\": true,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"ngrok_auth_token\": \"1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"experiment-meaningful-description-2020-04-23-18-26-20\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-655183167881/experiment-meaningful-description-2020-04-23-18-26-20/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_sagemaker\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_sagemaker.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"ngrok_auth_token\":\"1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV\",\"ngrok_daemon_start\":true,\"optimizer\":\"adam\",\"tensorboard_daemon_start\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_sagemaker.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_sagemaker\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-655183167881/experiment-meaningful-description-2020-04-23-18-26-20/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"ngrok_auth_token\":\"1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV\",\"ngrok_daemon_start\":true,\"optimizer\":\"adam\",\"tensorboard_daemon_start\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"experiment-meaningful-description-2020-04-23-18-26-20\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-655183167881/experiment-meaningful-description-2020-04-23-18-26-20/source/sourcedir.tar.gz\",\"module_name\":\"train_sagemaker\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_sagemaker.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--ngrok_auth_token\",\"1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV\",\"--ngrok_daemon_start\",\"True\",\"--optimizer\",\"adam\",\"--tensorboard_daemon_start\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_NGROK_DAEMON_START=true\u001b[0m\n",
      "\u001b[34mSM_HP_TENSORBOARD_DAEMON_START=true\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_NGROK_AUTH_TOKEN=1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m train_sagemaker --ngrok_auth_token 1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV --ngrok_daemon_start True --optimizer adam --tensorboard_daemon_start True\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m{\n",
      "    \"aug_geom_reflect\": false,\n",
      "    \"aug_geom_scale_max\": 1.0,\n",
      "    \"aug_geom_scale_min\": 1.0,\n",
      "    \"aug_geom_tilt_max_deg\": 0.0,\n",
      "    \"aug_geom_wiggle_max_ratio\": 0.0,\n",
      "    \"aug_input_crop_size\": 256,\n",
      "    \"batch_size\": 4,\n",
      "    \"batch_size_validation\": 8,\n",
      "    \"dataset\": \"miniscapes\",\n",
      "    \"dataset_root\": \"/opt/ml/input/data/training/miniscapes\",\n",
      "    \"log_dir\": \"/opt/ml/model/log\",\n",
      "    \"log_to_console\": true,\n",
      "    \"loss_weight_depth\": 0.5,\n",
      "    \"loss_weight_semseg\": 0.5,\n",
      "    \"lr_scheduler\": \"poly\",\n",
      "    \"lr_scheduler_power\": 0.9,\n",
      "    \"model_encoder_name\": \"resnet34\",\n",
      "    \"model_name\": \"deeplabv3p\",\n",
      "    \"ngrok_auth_token\": \"1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV\",\n",
      "    \"ngrok_daemon_start\": true,\n",
      "    \"num_epochs\": 16,\n",
      "    \"num_steps_visualization_first\": 100,\n",
      "    \"num_steps_visualization_interval\": 1000,\n",
      "    \"observe_train_ids\": [\n",
      "        0,\n",
      "        100\n",
      "    ],\n",
      "    \"observe_valid_ids\": [\n",
      "        0,\n",
      "        100\n",
      "    ],\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"optimizer_lr\": 0.01,\n",
      "    \"optimizer_momentum\": 0.9,\n",
      "    \"optimizer_weight_decay\": 0.0001,\n",
      "    \"prepare_submission\": false,\n",
      "    \"tensorboard_daemon_port\": 10000,\n",
      "    \"tensorboard_daemon_start\": true,\n",
      "    \"tensorboard_img_grid_width\": 8,\n",
      "    \"visualize_num_samples_in_batch\": 8,\n",
      "    \"workers\": 16,\n",
      "    \"workers_validation\": 4\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mNumber of samples in train split: 20000\u001b[0m\n",
      "\u001b[34mNumber of samples in val split: 2500\u001b[0m\n",
      "\u001b[34mNumber of samples in test split: 2500\u001b[0m\n",
      "\u001b[34mModelDeepLabV3Plus(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aspp): ASPP(\n",
      "    (conv_out): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (decoder): DecoderDeeplabV3p(\n",
      "    (features_to_predictions): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mgpu available: True, used: True\u001b[0m\n",
      "\u001b[34mVISIBLE GPUS: 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRunning TensorBoard daemon on port 10000\u001b[0m\n",
      "\u001b[34mRunning ngrok daemon forwarding from port 10000\u001b[0m\n",
      "\u001b[34mEpoch 00001: metric improved from -inf to -23.22693, saving model to /opt/ml/model/log/checkpoints/_ckpt_epoch_1.ckpt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "hyperparameters={\n",
    "    # enable tensorboard server to run alongside the training code (you need this to track progress)\n",
    "    'tensorboard_daemon_start': True,\n",
    "    # enable forwarding tensorboard to your ngrok.com account (you need this to access tensorboard)\n",
    "    'ngrok_daemon_start': True,\n",
    "    # TODO: use your personal ngrok.com account authtoken to access tensorboard\n",
    "    'ngrok_auth_token': '1atw4nNy5xp5bGrEuHJGSGFtOs5_71inK3qkTWZcHX1tbtfuV',\n",
    "    \n",
    "    # TODO: override configurable experiment parameters below this line\n",
    "    # 'batch_size': <new value>,\n",
    "    # 'num_epochs': ...,\n",
    "    # ...\n",
    "    'optimizer': 'adam',\n",
    "    'optimizer_lr': 0.001\n",
    "}\n",
    "\n",
    "# TODO: give name to distinct experiments, e.g. 'batch-size-4--newideaX-on--someparam-off'\n",
    "experiment_name = 'experiment-meaningful-description'\n",
    "\n",
    "#\n",
    "# no changes below this line\n",
    "#\n",
    "import datetime\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir='./',\n",
    "    entry_point='train_sagemaker.py',\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    framework_version='1.4.0',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    train_volume_size=30,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_run=86000,\n",
    "    train_max_wait=86400,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    {'training': 's3://dlad-miniscapes/miniscapes.zip'},\n",
    "    job_name=experiment_name + datetime.datetime.now().strftime(\"-%Y-%m-%d-%H-%M-%S\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
