{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DLAD Exercise 2: Multitask Learning\n",
    "\n",
    "### Working with code\n",
    "\n",
    "- fork the original exercise template repository under your user account, make sure that the fork remains private\n",
    "- [Set up an \"upstream\" remote of your fork](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/configuring-a-remote-for-a-fork) so that you can [pull in changes from the upstream repository](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/syncing-a-fork) (template solution you forked) whenever we make important updates (announced via Piazza/email)\n",
    "- add your fork to SageMaker repositories (see course presentation on AWS or check [here](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility/))\n",
    "- clone your fork to your PC, edit code, push changes back to your fork\n",
    "\n",
    "### Running experiments\n",
    "\n",
    "- The code is pulled from a linked repository _automatically_ only when you start the Notebook Instance. Subsequent pulls have to be done from the Terminal (see below)\n",
    "- The only cell in this notebook launches a new Training Job, and trains a model with it until the end\n",
    "- You can watch live TensorBoard every now and then to make sure training is doing fine. Just making sure the first training steps did not fail is enough in most cases\n",
    "- If you regret the hyperparameter choice, you can stop the job by visiting [SageMaker Training Jobs Console](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs), selecting the job in progress, and pressing Stop button. The job will take a couple of minutes to transition into Stopped state; until then your job is counted against your limits. After that you can stop the notebook cell. *Stopping the notebook cell alone does not terminate the Training Job, but only disconnects the notebook from it*\n",
    "- Each Training Job has a very simple lifecycle: it either runs until the end, thus resulting in a `submission.zip` file, or it fails/stops, in which case it cannot be resumed.\n",
    "\n",
    "### Incremental changes\n",
    "\n",
    "- (optional) With code changes\n",
    " - change code in your standalone environment\n",
    " - dry run code in your standalone environment to make sure it doesn't fail due to a syntax error, thus saving you time with Training Job initialization (order of 5 minutes)\n",
    " - push code to your private fork\n",
    " - open the Terminal within your Notebook Instance (Jupyter Logo in top-left corner -> New -> Terminal), navigate to your code folder\n",
    " - execute `git pull` to fetch the changes you pushed to your fork\n",
    "- (optional) With hyperparameter changes\n",
    " - update the hyperparameters dictionary in the cell below. It contains the command line parameters, which you would normally pass via command line as `--key <value>` in the standalone environment. All command line parameters are described in `mtl/utils/config.py`.\n",
    "- Assign a meaningful yet short name to your experiment. A good name allows you to quickly understand the meaning of the experiment when looking at a list with 20 other experiments. Example: `question1-lr-0.001-optimizer-adam`\n",
    "- Run the cell\n",
    "\n",
    "### Running multiple experiments simultaneously\n",
    "\n",
    "Each account is likely to have a limit of ml.p2.xlarge instances larger than 1, meaning that training multiple experiments in parallel is possible. To do that, we recommend having multiple (equal to your limit) separate Notebook Instances, each linked with the same repository. This way each of the Notebook Instances corresponds to one Training Job at time.\n",
    "\n",
    "## Start a Training Job\n",
    "\n",
    "- \"In [\\*]\" next to the cell means the Training Job is running\n",
    "- \"In [any digit]\" means the Trainin Job has finished, or the cell was disconnected from the job (either manually or by restarting/reconnecting to the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-27 09:41:41 Starting - Starting the training job...\n",
      "2020-05-27 09:41:44 Starting - Launching requested ML instances.........\n",
      "2020-05-27 09:43:28 Starting - Preparing the instances for training.........\n",
      "2020-05-27 09:44:43 Downloading - Downloading input data.........\n",
      "2020-05-27 09:46:18 Training - Downloading the training image........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-05-27 09:47:53,649 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-05-27 09:47:53,682 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-05-27 09:47:55,106 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-05-27 09:47:56,062 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpapgwp6le/module_dir\u001b[0m\n",
      "\u001b[34mCollecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.17.3\n",
      "  Downloading numpy-1.17.3-cp36-cp36m-manylinux1_x86_64.whl (20.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting Pillow==6.2.1\n",
      "  Downloading Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==0.5.2.1\n",
      "  Downloading pytorch-lightning-0.5.2.1.tar.gz (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting test-tube==0.7.3\n",
      "  Downloading test_tube-0.7.3.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.5.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.5.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.36.1\n",
      "  Downloading tqdm-4.36.1-py2.py3-none-any.whl (52 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.8.0-py3-none-any.whl (3.3 MB)\u001b[0m\n",
      "\n",
      "2020-05-27 09:47:52 Training - Training image download completed. Training in progress.\u001b[34mCollecting tb-nightly==1.15.0a20190708\n",
      "  Downloading tb_nightly-1.15.0a20190708-py3-none-any.whl (3.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==0.5.0->-r requirements.txt (line 7)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (2019.3)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34m  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.6.3\n",
      "  Downloading grpcio-1.29.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pytorch-lightning, test-tube, DLAD-ex2-multitask, absl-py\n",
      "  Building wheel for pytorch-lightning (setup.py): started\n",
      "  Building wheel for pytorch-lightning (setup.py): finished with status 'done'\n",
      "  Created wheel for pytorch-lightning: filename=pytorch_lightning-0.5.2.1-py3-none-any.whl size=62480 sha256=7a10a3c40b181c2e6a9d3dca5e4d3f15aae468e63462cd4c1cdbf50bb68603e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/25/4b/1d8ef595ef56b11055751ff6d70037c2bca8fe29ce097dee98\n",
      "  Building wheel for test-tube (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for test-tube (setup.py): finished with status 'done'\n",
      "  Created wheel for test-tube: filename=test_tube-0.7.3-py3-none-any.whl size=25049 sha256=5fe299660b72e9678bb5f8ebc4163edbf2179c547484b8e84f54264d5218ab56\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/00/f4/dd4f6697d4925bfec3cd7ff8fed88d9930496d562454f5e70d\n",
      "  Building wheel for DLAD-ex2-multitask (setup.py): started\n",
      "  Building wheel for DLAD-ex2-multitask (setup.py): finished with status 'done'\n",
      "  Created wheel for DLAD-ex2-multitask: filename=DLAD_ex2_multitask-1.0-py3-none-any.whl size=2067 sha256=9abe370e1b6e7f832e94e7e16f00ec8dd7efef29aecfecb067128320c66ec39a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jkp4gyaf/wheels/84/e9/45/f3f753dda4dda0e05960a99278b308a396fd5c27207e5397f5\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=c5c3d1878f4cae2366ac2c583c77eadb699f91c783e03967ee91de5954a39fc9\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[34mSuccessfully built pytorch-lightning test-tube DLAD-ex2-multitask absl-py\u001b[0m\n",
      "\u001b[34mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.36.1 which is incompatible.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, matplotlib, Pillow, tqdm, imageio, absl-py, markdown, grpcio, tb-nightly, test-tube, pytorch-lightning, DLAD-ex2-multitask\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.2.1\n",
      "    Uninstalling matplotlib-3.2.1:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled matplotlib-3.2.1\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 7.1.0\n",
      "    Uninstalling Pillow-7.1.0:\n",
      "      Successfully uninstalled Pillow-7.1.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed DLAD-ex2-multitask-1.0 Pillow-6.2.1 absl-py-0.9.0 grpcio-1.29.0 imageio-2.8.0 markdown-3.2.2 matplotlib-3.1.1 numpy-1.17.3 pytorch-lightning-0.5.2.1 tb-nightly-1.15.0a20190708 test-tube-0.7.3 tqdm-4.36.1\u001b[0m\n",
      "\u001b[34m2020-05-27 09:48:13,512 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib==3.1.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy==1.17.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow==6.2.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (6.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytorch-lightning==0.5.2.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.5.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: test-tube==0.7.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==1.4.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision==0.5.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (0.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm==4.36.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib==3.1.1->-r requirements.txt (line 1)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (0.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tb-nightly==1.15.0a20190708 in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (1.15.0a20190708)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from test-tube==0.7.3->-r requirements.txt (line 5)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from torchvision==0.5.0->-r requirements.txt (line 7)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->pytorch-lightning==0.5.2.1->-r requirements.txt (line 4)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.11.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (46.1.3.post20200330)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (0.34.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.29.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from markdown>=2.6.8->tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly==1.15.0a20190708->test-tube==0.7.3->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: DLAD-ex2-multitask\n",
      "  Building wheel for DLAD-ex2-multitask (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for DLAD-ex2-multitask (setup.py): finished with status 'done'\n",
      "  Created wheel for DLAD-ex2-multitask: filename=DLAD_ex2_multitask-1.0-py3-none-any.whl size=2067 sha256=a1d83d7b08c473056ab84c65affa5df464886a7e9a3ed56cac185d386abe553d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-27xm6848/wheels/95/c1/85/65aaf48b35aba88c6e896d2fd04a4b69f1cee0d81ea32993ca\u001b[0m\n",
      "\u001b[34mSuccessfully built DLAD-ex2-multitask\u001b[0m\n",
      "\u001b[34mInstalling collected packages: DLAD-ex2-multitask\n",
      "  Attempting uninstall: DLAD-ex2-multitask\n",
      "    Found existing installation: DLAD-ex2-multitask 1.0\n",
      "    Uninstalling DLAD-ex2-multitask-1.0:\n",
      "      Successfully uninstalled DLAD-ex2-multitask-1.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed DLAD-ex2-multitask-1.0\u001b[0m\n",
      "\u001b[34m2020-05-27 09:48:15,609 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 16,\n",
      "        \"ngrok_daemon_start\": true,\n",
      "        \"tensorboard_daemon_start\": true,\n",
      "        \"ngrok_auth_token\": \"1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN\",\n",
      "        \"aug_input_crop_size\": 512,\n",
      "        \"loss_weight_semseg\": 0.6,\n",
      "        \"loss_weight_depth\": 0.4,\n",
      "        \"model_name\": \"distillation\",\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"optimizer_lr\": 0.0001\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"Problem4-cropsize512-2020-05-27-09-41-34\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-655183167881/Problem4-cropsize512-2020-05-27-09-41-34/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_sagemaker\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_sagemaker.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"aug_input_crop_size\":512,\"batch_size\":16,\"loss_weight_depth\":0.4,\"loss_weight_semseg\":0.6,\"model_name\":\"distillation\",\"ngrok_auth_token\":\"1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN\",\"ngrok_daemon_start\":true,\"optimizer\":\"adam\",\"optimizer_lr\":0.0001,\"tensorboard_daemon_start\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_sagemaker.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_sagemaker\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-655183167881/Problem4-cropsize512-2020-05-27-09-41-34/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"aug_input_crop_size\":512,\"batch_size\":16,\"loss_weight_depth\":0.4,\"loss_weight_semseg\":0.6,\"model_name\":\"distillation\",\"ngrok_auth_token\":\"1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN\",\"ngrok_daemon_start\":true,\"optimizer\":\"adam\",\"optimizer_lr\":0.0001,\"tensorboard_daemon_start\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"Problem4-cropsize512-2020-05-27-09-41-34\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-655183167881/Problem4-cropsize512-2020-05-27-09-41-34/source/sourcedir.tar.gz\",\"module_name\":\"train_sagemaker\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_sagemaker.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--aug_input_crop_size\",\"512\",\"--batch_size\",\"16\",\"--loss_weight_depth\",\"0.4\",\"--loss_weight_semseg\",\"0.6\",\"--model_name\",\"distillation\",\"--ngrok_auth_token\",\"1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN\",\"--ngrok_daemon_start\",\"True\",\"--optimizer\",\"adam\",\"--optimizer_lr\",\"0.0001\",\"--tensorboard_daemon_start\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_NGROK_DAEMON_START=true\u001b[0m\n",
      "\u001b[34mSM_HP_TENSORBOARD_DAEMON_START=true\u001b[0m\n",
      "\u001b[34mSM_HP_NGROK_AUTH_TOKEN=1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN\u001b[0m\n",
      "\u001b[34mSM_HP_AUG_INPUT_CROP_SIZE=512\u001b[0m\n",
      "\u001b[34mSM_HP_LOSS_WEIGHT_SEMSEG=0.6\u001b[0m\n",
      "\u001b[34mSM_HP_LOSS_WEIGHT_DEPTH=0.4\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distillation\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER_LR=0.0001\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m train_sagemaker --aug_input_crop_size 512 --batch_size 16 --loss_weight_depth 0.4 --loss_weight_semseg 0.6 --model_name distillation --ngrok_auth_token 1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN --ngrok_daemon_start True --optimizer adam --optimizer_lr 0.0001 --tensorboard_daemon_start True\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m{\n",
      "    \"aug_geom_reflect\": false,\n",
      "    \"aug_geom_scale_max\": 1.0,\n",
      "    \"aug_geom_scale_min\": 1.0,\n",
      "    \"aug_geom_tilt_max_deg\": 0.0,\n",
      "    \"aug_geom_wiggle_max_ratio\": 0.0,\n",
      "    \"aug_input_crop_size\": 512,\n",
      "    \"batch_size\": 16,\n",
      "    \"batch_size_validation\": 8,\n",
      "    \"dataset\": \"miniscapes\",\n",
      "    \"dataset_root\": \"/opt/ml/input/data/training/miniscapes\",\n",
      "    \"log_dir\": \"/opt/ml/model/log\",\n",
      "    \"log_to_console\": true,\n",
      "    \"loss_weight_depth\": 0.4,\n",
      "    \"loss_weight_semseg\": 0.6,\n",
      "    \"lr_scheduler\": \"poly\",\n",
      "    \"lr_scheduler_power\": 0.9,\n",
      "    \"model_encoder_name\": \"resnet34\",\n",
      "    \"model_name\": \"distillation\",\n",
      "    \"ngrok_auth_token\": \"1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN\",\n",
      "    \"ngrok_daemon_start\": true,\n",
      "    \"num_epochs\": 16,\n",
      "    \"num_steps_visualization_first\": 100,\n",
      "    \"num_steps_visualization_interval\": 1000,\n",
      "    \"observe_train_ids\": [\n",
      "        0,\n",
      "        100\n",
      "    ],\n",
      "    \"observe_valid_ids\": [\n",
      "        0,\n",
      "        100\n",
      "    ],\n",
      "    \"optimizer\": \"adam\",\n",
      "    \"optimizer_lr\": 0.0001,\n",
      "    \"optimizer_momentum\": 0.9,\n",
      "    \"optimizer_weight_decay\": 0.0001,\n",
      "    \"prepare_submission\": false,\n",
      "    \"tensorboard_daemon_port\": 10000,\n",
      "    \"tensorboard_daemon_start\": true,\n",
      "    \"tensorboard_img_grid_width\": 8,\n",
      "    \"visualize_num_samples_in_batch\": 8,\n",
      "    \"workers\": 16,\n",
      "    \"workers_validation\": 4\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mNumber of samples in train split: 20000\u001b[0m\n",
      "\u001b[34mNumber of samples in val split: 2500\u001b[0m\n",
      "\u001b[34mNumber of samples in test split: 2500\u001b[0m\n",
      "\u001b[34moutputs_desc {'semseg': 19, 'depth': 1}\u001b[0m\n",
      "\u001b[34mModelDistillation(\n",
      "  (encoder): Encoder(\n",
      "    (encoder): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlockWithDilation(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aspp_semseg): ASPP(\n",
      "    (conv1x1): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_rate3): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_rate6): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_rate9): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_out): ASPPpart(\n",
      "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv1x1_global): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (global_average_pooling): AvgPool2d(kernel_size=(37, 49), stride=(37, 49), padding=0)\n",
      "  )\n",
      "  (decoder_semseg): DecoderDeeplabV3p(\n",
      "    (conv1x1_low_level_features): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final1): Sequential(\n",
      "      (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final3): Conv2d(256, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (assp_depth): ASPP(\n",
      "    (conv1x1): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_rate3): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_rate6): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_rate9): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(9, 9), dilation=(9, 9), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv_out): ASPPpart(\n",
      "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv1x1_global): ASPPpart(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (global_average_pooling): AvgPool2d(kernel_size=(37, 49), stride=(37, 49), padding=0)\n",
      "  )\n",
      "  (decoder_depth): DecoderDeeplabV3p(\n",
      "    (conv1x1_low_level_features): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final1): Sequential(\n",
      "      (0): Conv2d(320, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final3): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (sa_from_depth2semseg): SelfAttention(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (attention): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (sa_from_semseg2depth): SelfAttention(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (attention): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (decoder_semseg2): DecoderDistillation(\n",
      "    (conv3x3_final1): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final3): Conv2d(256, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (decoder_depth2): DecoderDistillation(\n",
      "    (conv3x3_final1): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv3x3_final3): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mNumber of parameters: 37025856\u001b[0m\n",
      "\u001b[34mgpu available: True, used: True\u001b[0m\n",
      "\u001b[34mVISIBLE GPUS: 0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRunning TensorBoard daemon on port 10000\u001b[0m\n",
      "\u001b[34mRunning ngrok daemon forwarding from port 10000\u001b[0m\n",
      "\u001b[34m2020-05-27 09:49:20,994 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python -m train_sagemaker --aug_input_crop_size 512 --batch_size 16 --loss_weight_depth 0.4 --loss_weight_semseg 0.6 --model_name distillation --ngrok_auth_token 1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN --ngrok_daemon_start True --optimizer adam --optimizer_lr 0.0001 --tensorboard_daemon_start True\"\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0.00/83.3M [00:00<?, ?B/s]#015 26%|       | 21.4M/83.3M [00:00<00:00, 225MB/s]#015 52%|    | 43.5M/83.3M [00:00<00:00, 227MB/s]#015 79%|  | 66.2M/83.3M [00:00<00:00, 230MB/s]#015100%|| 83.3M/83.3M [00:00<00:00, 232MB/s]\u001b[0m\n",
      "\u001b[34m#0150it [00:00, ?it/s]#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|| 1/1 [00:04<00:00,  4.56s/it]#015  0%|          | 0/1563 [00:00<1:58:41,  4.56s/it]#015  0%|          | 1/1563 [00:20<4:04:59,  9.41s/it]type=<class 'RuntimeError'> value=CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 10.39 GiB already allocated; 37.31 MiB free; 10.85 GiB reserved in total by PyTorch) traceback=['  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\\n    \"__main__\", mod_spec)\\n', '  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\\n    exec(code, run_globals)\\n', '  File \"/opt/ml/code/train_sagemaker.py\", line 59, in <module>\\n    main()\\n', '  File \"/opt/ml/code/mtl/scripts/train.py\", line 67, in main\\n    trainer.fit(model)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 754, in fit\\n    self.__single_gpu_train(model)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 802, in __single_gpu_train\\n    self.__run_pretrain_routine(model)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1048, in __run_pretrain_routine\\n    self.__train()\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1080, in __train\\n    self.run_training_epoch()\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1124, in run_training_epoch\\n    output = self.__run_training_batch(batch, batch_nb)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1425, in __run_training_batch\\n    loss = optimizer_closure()\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1396, in optimizer_closure\\n    output = self.__training_forward(batch, batch_nb, opt_idx)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 1268, in __training_forward\\n    output = self.model.training_step(*args)\\n', '  File \"/opt/ml/code/mtl/experiments/experiment_semseg_with_depth.py\", line 89, in training_step\\n    y_hat = self.net(rgb)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 543, in __call__\\n    result = self.forward(*input, **kwargs)\\n', '  File \"/opt/ml/code/mtl/models/model_distillation.py\", line 66, in forward\\n    attention_from_depth2semseg = self.sa_from_depth2semseg(features_before_final_layer_depth)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 543, in __call__\\n    result = self.forward(*input, **kwargs)\\n', '  File \"/opt/ml/code/mtl/models/model_parts.py\", line 238, in forward\\n    attention_mask = torch.sigmoid(self.attention(x))\\n', '  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 543, in __call__\\n    result = self.forward(*input, **kwargs)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 345, in forward\\n    return self.conv2d_forward(input, self.weight)\\n', '  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 342, in conv2d_forward\\n    self.padding, self.dilation, self.groups)\\n']\n",
      "\u001b[0m\n",
      "\u001b[34m#015  0%|          | 1/1563 [00:22<9:46:13, 22.52s/it]\u001b[0m\n",
      "\n",
      "2020-05-27 09:49:29 Uploading - Uploading generated training model\n",
      "2020-05-27 09:49:29 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job Problem4-cropsize512-2020-05-27-09-41-34: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python -m train_sagemaker --aug_input_crop_size 512 --batch_size 16 --loss_weight_depth 0.4 --loss_weight_semseg 0.6 --model_name distillation --ngrok_auth_token 1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN --ngrok_daemon_start True --optimizer adam --optimizer_lr 0.0001 --tensorboard_daemon_start True\"\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-484674813ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m estimator.fit(\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m's3://dlad-miniscapes/miniscapes.zip'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-%Y-%m-%d-%H-%M-%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3045\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2636\u001b[0m                 ),\n\u001b[1;32m   2637\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2638\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2639\u001b[0m             )\n\u001b[1;32m   2640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job Problem4-cropsize512-2020-05-27-09-41-34: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python -m train_sagemaker --aug_input_crop_size 512 --batch_size 16 --loss_weight_depth 0.4 --loss_weight_semseg 0.6 --model_name distillation --ngrok_auth_token 1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN --ngrok_daemon_start True --optimizer adam --optimizer_lr 0.0001 --tensorboard_daemon_start True\"\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboar"
     ]
    }
   ],
   "source": [
    "hyperparameters={\n",
    "    # enable tensorboard server to run alongside the training code (you need this to track progress)\n",
    "    'tensorboard_daemon_start': True,\n",
    "    # enable forwarding tensorboard to your ngrok.com account (you need this to access tensorboard)\n",
    "    'ngrok_daemon_start': True,\n",
    "    # TODO: use your personal ngrok.com account authtoken to access tensorboard\n",
    "    'ngrok_auth_token': '1afOlP4NIiVdFYkxUIrEctjBL0O_4NpsyRhTMoQt1zdTXugyN',\n",
    "    \n",
    "    # TODO: override configurable experiment parameters below this line\n",
    "    # 'batch_size': <new value>,\n",
    "    # 'num_epochs': ...,\n",
    "    # ...\n",
    "    'optimizer': 'adam',\n",
    "    'optimizer_lr': 0.0001,\n",
    "    'batch_size': 16,\n",
    "    'loss_weight_semseg': 0.6,\n",
    "    'loss_weight_depth': 0.4,\n",
    "    'model_name': 'distillation',\n",
    "    'aug_input_crop_size': 512    \n",
    "    #'aug_geom_scale_min': 0.5,\n",
    "    #'aug_geom_scale_max': 1.0,\n",
    "    #'aug_geom_tilt_max_deg': 0.33,\n",
    "    #'aug_geom_wiggle_max_ratio': 0.33,\n",
    "    #'aug_geom_reflect': True,\n",
    "\n",
    "}\n",
    "\n",
    "# TODO: give name to distinct experiments, e.g. 'batch-size-4--newideaX-on--someparam-off'\n",
    "experiment_name = 'Problem4-cropsize512'\n",
    "\n",
    "#\n",
    "# no changes below this line\n",
    "#\n",
    "import datetime\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir='./',\n",
    "    entry_point='train_sagemaker.py',\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    framework_version='1.4.0',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    train_volume_size=30,\n",
    "    train_use_spot_instances=True,\n",
    "    train_max_run=86000,\n",
    "    train_max_wait=86400,\n",
    "    debugger_hook_config=False,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    {'training': 's3://dlad-miniscapes/miniscapes.zip'},\n",
    "    job_name=experiment_name + datetime.datetime.now().strftime(\"-%Y-%m-%d-%H-%M-%S\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
